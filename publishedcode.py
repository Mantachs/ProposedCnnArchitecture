# -*- coding: utf-8 -*-
"""PublishedCode.ipynb

Automatically generated by Colaboratory.

The code present here represent the architecutre that was proposed in the following paper:
Mantach S, Ashraf A, Janani H, Kordi B. A Convolutional Neural Network-Based Model for Multi-Source and Single-Source Partial Discharge Pattern Classification Using Only Single-Source Training Set. Energies. 2021 Jan;14(5):1355.

The  main  contribution  of  this project  includes  classifying  multi-source  PDs  andsingle-source  PDs  without  introducing  multi-source  PDs  in  the  training  stage.   The motivation of this work is that collecting multi-source PDs is difficult, expensive, time consuming , and not practical in regards to the combinational nature of the PDs.  The deep learning algorithm investigated here is two dimensional convolutional neural network.  The proposed architecture is different than traditional CNN architectures, where it consists of two parts: the convolutional layers which take care of learning generalized features of all the classes, and the fully connected layers which take care of learning the fine-tuned specific features of each class.  Extensive validation of the proposed neural network architecture has been conducted by evaluating the generalization in performance as we move from training data (i.e.  single source) to unseen testing data whichis multi-source.

As mentioned in the paper, the input to this network is a 100 by 100 matrix representing the PRPD pattern for a given data point. 
Background noise in PRPDs is reflected by having an offset charge over all the phase windows.  In this work, it has been removed for all the PRPD patterns.
An additional no-pattern class corresponding to the cases not involving the presence of any PD is added.  
In order to encourage the model to learn features related to the shape of PRPDs, the samples were converted into binary samples, where zero threshold is considered for binarization.

Functional keras was used for the implementation. 
Since there are six classes , in addition to the no-pattern class, the resultant output has seven nodes. 
The loss weights in compile is related to including the effect of each class on the decision making' in other words, on the parameters update. In this work, the weights were used in regards to the used examples in order to have same effect of all classes on the decision making.
"""

shuffle=False

input1 = keras.Input(shape=(100,100, 1))
x = BatchNormalization(       )( input1 )
x = layers.Conv2D(36,3, padding="same", kernel_initializer = 'he_normal')(x)
x=MaxPooling2D(2)(x)
x = BatchNormalization(       )( x )
x =         Activation('relu' )( x )

x = layers.Conv2D(36,3, padding="same", kernel_initializer = 'he_normal')(x)
x=MaxPooling2D(2)(x)
x = BatchNormalization(       )( x )
x =         Activation('relu' )( x )

x = layers.Flatten()  (x)
x = layers.Dense(128, kernel_initializer = 'he_normal')(x)
x = BatchNormalization(       )( x )
x =         Activation('relu' )( x )

x = layers.Dense(64, kernel_initializer = 'he_normal')(x)
x = BatchNormalization(       )( x )
x =         Activation('relu' )( x )

x = layers.Dense(7, kernel_initializer = 'he_normal')(x)
outputs =         Activation('sigmoid' )( x )
model  = keras.Model(  inputs  = [input1] , outputs = [ outputs[:,0], outputs[:,1],outputs[:,2],outputs[:,3],outputs[:,4],outputs[:,5],outputs[:,6]])
bc = "binary_crossentropy"
opt = tf.keras.optimizers.Adam(lr=0.0001, decay=1e-4)
model.compile(optimizer= opt , loss = [bc,bc,bc,bc,bc,bc,bc], loss_weights = [120/250,120/290,120/130, 120/182, 120/120,120/350,120/200] )
model.summary()
history= model.fit(X, [trainy[:,0],trainy[:,1],trainy[:,2],trainy[:,3],trainy[:,4],trainy[:,5],trainy[:,6]] , 
                   batch_size=len(trainy),epochs=4000 , validation_split=0.1,shuffle=False)
